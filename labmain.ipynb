{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "from labfuncs import read_books, train_model, classify\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read books for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "\n",
      "Train corpa\n",
      "\n",
      "Author: Lewis Carroll\n",
      "        Book: The Hunting of the Snark - An Agony in Eight Fits\n",
      "        Book: Sylvie and Bruno\n",
      "        Book: The Game of Logic\n",
      "        Book: Alice_s Adventures in Wonderland\n",
      "        Book: Phantasmagoria and Other Poems\n",
      "        Book: Through the Looking-Glass\n",
      "        Book: Symbolic Logic\n",
      "Author: Mark Twain\n",
      "        Book: The Mysterious Stranger, and Other Stories\n",
      "        Book: The Prince and the Pauper\n",
      "        Book: Eve_s Diary, Complete\n",
      "        Book: Adventures of Huckleberry Finn\n",
      "        Book: Roughing It\n",
      "Author: Jane Austen\n",
      "        Book: Emma\n",
      "        Book: Sense and Sensibility\n",
      "        Book: Mansfield Park\n",
      "        Book: Persuasion\n",
      "        Book: Northanger Abbey\n",
      "        Book: Lady Susan\n",
      "Author: Arthur Conan Doyle\n",
      "        Book: The Valley of Fear\n",
      "        Book: The Return of Sherlock Holmes\n",
      "        Book: The Lost World\n",
      "        Book: The Memoirs of Sherlock Holmes\n",
      "        Book: Tales of Terror and Mystery\n",
      "        Book: The Adventures of Sherlock Holmes\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test corpa\n",
      "\n",
      "Title: Pride and Prejudice\n",
      "Title: Alice_s Adventures Under Ground\n",
      "Title: The Adventures of Tom Sawyer\n",
      "Title: The Hound of the Baskervilles \n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------------')\n",
    "print()\n",
    "print('Train corpa')\n",
    "print()\n",
    "trainbooks = read_books('books')\n",
    "print()\n",
    "print('----------------------------------------------------------------')\n",
    "print()\n",
    "print('Test corpa')\n",
    "print()\n",
    "testbooks = read_books('testbooks')\n",
    "print()\n",
    "print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make (sentence, author) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, sentences = [], []\n",
    "for author, works in trainbooks.items():\n",
    "    for title in works.keys():\n",
    "        for s in works[title]:\n",
    "            labels.append(author)\n",
    "            sentences.append(s)\n",
    "train = pandas.DataFrame()\n",
    "train['text'] = sentences\n",
    "train['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into train/validate subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['text'], train['label'])\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from texts: word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train['text'])\n",
    "xtrain_count = count_vect.transform(train_x)\n",
    "xvalid_count = count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from texts: word-level TF/IDF (Term Frequency and Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(train['text'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features form text: character-level ngrams TF/IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_char.fit(train['text'])\n",
    "xtrain_tfidf_char =  tfidf_vect_char.transform(train_x)\n",
    "xvalid_tfidf_char =  tfidf_vect_char.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classification based on word countings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  0.7679919663778034\n"
     ]
    }
   ],
   "source": [
    "count_word_classifier = naive_bayes.MultinomialNB()\n",
    "print(\"Naive Bayes, Count Vectors: \", train_model(count_word_classifier, xtrain_count, train_y, xvalid_count, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classification based on word-level TF/IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Word-level TF/IDF:  0.7387585078290624\n"
     ]
    }
   ],
   "source": [
    "tfidf_word_classifier = naive_bayes.MultinomialNB()\n",
    "print(\"Naive Bayes, Word-level TF/IDF: \", train_model(tfidf_word_classifier, xtrain_tfidf, train_y, xvalid_tfidf, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier based on word-level TF/IDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear, word-level TF/IDF:  0.7736824487670622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "tfidf_linear_classifier = linear_model.LogisticRegression()\n",
    "print(\"Linear, word-level TF/IDF: \", train_model(tfidf_linear_classifier, xtrain_tfidf, train_y, xvalid_tfidf, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier based on char-level ngrams TF/IDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear, word-level TF/IDF:  0.720682857886711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "tfidf_linear_char_classifier = linear_model.LogisticRegression()\n",
    "print(\"Linear, word-level TF/IDF: \", train_model(tfidf_linear_char_classifier, xtrain_tfidf_char, train_y, xvalid_tfidf_char, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"Alice's Adventures Under Ground\"\n",
      "   57.65%  -  Lewis Carroll\n",
      "   22.89%  -  Jane Austen\n",
      "   12.45%  -  Arthur Conan Doyle\n",
      "    7.01%  -  Mark Twain\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"Pride and Prejudice\"\n",
      "   96.83%  -  Jane Austen\n",
      "    3.03%  -  Arthur Conan Doyle\n",
      "    0.10%  -  Mark Twain\n",
      "    0.04%  -  Lewis Carroll\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"The Hound of the Baskervilles \"\n",
      "   90.96%  -  Arthur Conan Doyle\n",
      "    8.57%  -  Jane Austen\n",
      "    0.46%  -  Mark Twain\n",
      "    0.01%  -  Lewis Carroll\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"The Adventures of Tom Sawyer\"\n",
      "   77.12%  -  Mark Twain\n",
      "   14.43%  -  Arthur Conan Doyle\n",
      "    7.92%  -  Jane Austen\n",
      "    0.54%  -  Lewis Carroll\n"
     ]
    }
   ],
   "source": [
    "for title, sentences in testbooks.items():\n",
    "    print()\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(f'\\nTest title:  \"{title}\"')\n",
    "    probs = classify(sentences,\n",
    "                     [count_word_classifier, tfidf_word_classifier,\n",
    "                      tfidf_linear_classifier, tfidf_linear_char_classifier],\n",
    "                     [count_vect, tfidf_vect, tfidf_vect, tfidf_vect_char],\n",
    "                     method='intersect')\n",
    "    for prob, label in probs:\n",
    "        print(f'{prob * 100: 8.2f}%  -  {encoder.inverse_transform([label])[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
