{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c35199ec8954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "from labfuncs import read_books, train_model, classify\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/48/e9fa9e252abcd1447eff6f9257636af31758a6e46fd5ce5d3c879f6907cb/scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 91kB/s  eta 0:00:01    62% |████████████████████▏           | 4.4MB 11.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.11.0 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.2MB 31kB/s  eta 0:00:01    79% |█████████████████████████▎      | 15.9MB 11.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.17.0 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 26.1MB 24kB/s eta 0:00:011  3% |█▎                              | 1.0MB 11.5MB/s eta 0:00:03    66% |█████████████████████▍          | 17.4MB 11.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/vitalii/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, numpy, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.14.1 numpy-1.18.1 scikit-learn-0.22.1 scipy-1.4.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read books for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "\n",
      "Train corpa\n",
      "\n",
      "Author: Arthur Conan Doyle\n",
      "        Book: The Lost World\n",
      "        Book: Tales of Terror and Mystery\n",
      "        Book: The Valley of Fear\n",
      "        Book: The Return of Sherlock Holmes\n",
      "        Book: The Adventures of Sherlock Holmes\n",
      "        Book: The Memoirs of Sherlock Holmes\n",
      "Author: Jane Austen\n",
      "        Book: Sense and Sensibility\n",
      "        Book: Northanger Abbey\n",
      "        Book: Emma\n",
      "        Book: Persuasion\n",
      "        Book: Lady Susan\n",
      "        Book: Mansfield Park\n",
      "Author: Mark Twain\n",
      "        Book: Eve's Diary, Complete\n",
      "        Book: Adventures of Huckleberry Finn\n",
      "        Book: Roughing It\n",
      "        Book: The Mysterious Stranger, and Other Stories\n",
      "        Book: The Prince and the Pauper\n",
      "Author: Lewis Carroll\n",
      "        Book: Through the Looking-Glass\n",
      "        Book: Symbolic Logic\n",
      "        Book: Phantasmagoria and Other Poems\n",
      "        Book: The Hunting of the Snark - An Agony in Eight Fits\n",
      "        Book: Alice's Adventures in Wonderland\n",
      "        Book: Sylvie and Bruno\n",
      "        Book: The Game of Logic\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test corpa\n",
      "\n",
      "Title: Alice's Adventures Under Ground\n",
      "Title: Pride and Prejudice\n",
      "Title: The Hound of the Baskervilles \n",
      "Title: The Adventures of Tom Sawyer\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------------')\n",
    "print()\n",
    "print('Train corpa')\n",
    "print()\n",
    "trainbooks = read_books('books')\n",
    "print()\n",
    "print('----------------------------------------------------------------')\n",
    "print()\n",
    "print('Test corpa')\n",
    "print()\n",
    "testbooks = read_books('testbooks')\n",
    "print()\n",
    "print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make (sentence, author) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, sentences = [], []\n",
    "for author, works in trainbooks.items():\n",
    "    for title in works.keys():\n",
    "        for s in works[title]:\n",
    "            labels.append(author)\n",
    "            sentences.append(s)\n",
    "train = pandas.DataFrame()\n",
    "train['text'] = sentences\n",
    "train['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into train/validate subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['text'], train['label'])\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from texts: word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train['text'])\n",
    "xtrain_count = count_vect.transform(train_x)\n",
    "xvalid_count = count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from texts: word-level TF/IDF (Term Frequency and Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(train['text'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features form text: character-level ngrams TF/IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_char.fit(train['text'])\n",
    "xtrain_tfidf_char =  tfidf_vect_char.transform(train_x)\n",
    "xvalid_tfidf_char =  tfidf_vect_char.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classification based on word countings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  0.76732249786142\n"
     ]
    }
   ],
   "source": [
    "count_word_classifier = naive_bayes.MultinomialNB()\n",
    "print(\"Naive Bayes, Count Vectors: \", train_model(count_word_classifier, xtrain_count, train_y, xvalid_count, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classification based on word-level TF/IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Word-level TF/IDF:  0.7359318629821103\n"
     ]
    }
   ],
   "source": [
    "tfidf_word_classifier = naive_bayes.MultinomialNB()\n",
    "print(\"Naive Bayes, Word-level TF/IDF: \", train_model(tfidf_word_classifier, xtrain_tfidf, train_y, xvalid_tfidf, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier based on word-level TF/IDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/Documents/Lab/labEnv/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/vitalii/Documents/Lab/labEnv/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear, word-level TF/IDF:  0.7674712686428385\n"
     ]
    }
   ],
   "source": [
    "tfidf_linear_classifier = linear_model.LogisticRegression()\n",
    "print(\"Linear, word-level TF/IDF: \", train_model(tfidf_linear_classifier, xtrain_tfidf, train_y, xvalid_tfidf, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier based on char-level ngrams TF/IDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear, word-level TF/IDF:  0.7181909472979506\n"
     ]
    }
   ],
   "source": [
    "tfidf_linear_char_classifier = linear_model.LogisticRegression()\n",
    "print(\"Linear, word-level TF/IDF: \", train_model(tfidf_linear_char_classifier, xtrain_tfidf_char, train_y, xvalid_tfidf_char, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"Alice's Adventures Under Ground\"\n",
      "   57.65%  -  Lewis Carroll\n",
      "   22.89%  -  Jane Austen\n",
      "   12.45%  -  Arthur Conan Doyle\n",
      "    7.01%  -  Mark Twain\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"Pride and Prejudice\"\n",
      "   96.83%  -  Jane Austen\n",
      "    3.03%  -  Arthur Conan Doyle\n",
      "    0.10%  -  Mark Twain\n",
      "    0.04%  -  Lewis Carroll\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"The Hound of the Baskervilles \"\n",
      "   90.96%  -  Arthur Conan Doyle\n",
      "    8.57%  -  Jane Austen\n",
      "    0.46%  -  Mark Twain\n",
      "    0.01%  -  Lewis Carroll\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Test title:  \"The Adventures of Tom Sawyer\"\n",
      "   77.12%  -  Mark Twain\n",
      "   14.43%  -  Arthur Conan Doyle\n",
      "    7.92%  -  Jane Austen\n",
      "    0.54%  -  Lewis Carroll\n"
     ]
    }
   ],
   "source": [
    "for title, sentences in testbooks.items():\n",
    "    print()\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(f'\\nTest title:  \"{title}\"')\n",
    "    probs = classify(sentences,\n",
    "                     [count_word_classifier, tfidf_word_classifier,\n",
    "                      tfidf_linear_classifier, tfidf_linear_char_classifier],\n",
    "                     [count_vect, tfidf_vect, tfidf_vect, tfidf_vect_char],\n",
    "                     method='intersect')\n",
    "    for prob, label in probs:\n",
    "        print(f'{prob * 100: 8.2f}%  -  {encoder.inverse_transform([label])[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
